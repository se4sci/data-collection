{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huyqt7/Projects/PhD/data-collection/labeled_commits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shlex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess as sp\n",
    "import understand as und\n",
    "from pathlib import Path\n",
    "from pdb import set_trace\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "main_path = os.getcwd()\n",
    "p_df = {}\n",
    "print(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_LINKS = {\n",
    "    \"abinit\": {\n",
    "        \"url\": \"https://github.com/abinit/abinit\",\n",
    "        \"lang\": \"fortran\"\n",
    "    },\n",
    "    \"mdanalysis\": {\n",
    "        \"url\": \"https://github.com/MDAnalysis/mdanalysis\",\n",
    "        \"lang\": \"python\"\n",
    "    },\n",
    "    \"libmesh\": {\n",
    "        \"url\": \"https://github.com/libMesh/libmesh\",\n",
    "        \"lang\": \"C\"\n",
    "    },\n",
    "    \"lammps\": {\n",
    "        \"url\": \"https://github.com/lammps/lammps\",\n",
    "        \"lang\": \"C\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#projects = ['pcmsolver', 'rmg-py', 'hoomd', 'amber', 'xenon', 'mdanalysis', 'libmesh', 'abinit', 'lammps']\n",
    "def get_releases():\n",
    "    p_df = {}\n",
    "    for p in projects:\n",
    "        print(p)\n",
    "        project_path = \"/home/huyqt7/Projects/PhD/eager/data/projects/%s/\" % p\n",
    "        os.chdir(project_path)\n",
    "        #sp.check_output(\"git pull\", shell=True)\n",
    "        sp.check_output(\"git reset --hard master\", shell=True)\n",
    "        #sp.check_output(\"git pull\", shell=True)\n",
    "        versions = sp.check_output(\"git tag --sort=-taggerdate\", shell=True)\n",
    "        versions = str(versions)[2:-1]\n",
    "        versions = versions.split('\\\\n')\n",
    "        versions = [v for v in versions if v]\n",
    "        #print(versions)\n",
    "        re_commits = []\n",
    "        p_df[p] = {}\n",
    "        for ver in versions:\n",
    "            v_report = sp.check_output(\n",
    "                    \"git log -1 --pretty=format:\\\"%%H;%%ad\\\" %s\" % ver,\n",
    "                    shell=True)\n",
    "            re_commits.append(str(v_report)[2:-1].split(\";\"))\n",
    "        #print(re_commits)\n",
    "        p_df[p] = re_commits\n",
    "    return p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "def get_ground_truths():\n",
    "    p_df = {}\n",
    "    for p in projects:\n",
    "        files = [filename for filename in os.listdir(p) if filename.endswith(\".csv\")]\n",
    "        dfs = []\n",
    "        for f in files:\n",
    "            dfs.append(pd.read_csv(p + \"/\" + f, index_col=None))\n",
    "\n",
    "        all_commits = pd.concat(dfs)\n",
    "        \n",
    "        all_commits['time'] = pd.to_datetime(all_commits['time'])\n",
    "        all_commits = all_commits.drop_duplicates(subset=['hash'], keep=\"last\")\n",
    "        all_commits = all_commits.sort_values(by='time').reset_index()\n",
    "        print(all_commits.shape[0])\n",
    "        p_df[p] = all_commits\n",
    "        all_commits.to_csv(\"ugly1_\" + p + \".csv\", index=False)\n",
    "    return p_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdanalysis\n",
      "libmesh\n",
      "lammps\n"
     ]
    }
   ],
   "source": [
    "p_df = get_releases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3344, 6)\n",
      "(8679, 6)\n",
      "(7514, 6)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(main_path)\n",
    "#truth_df = get_ground_truths()\n",
    "truth_df = {}\n",
    "for p in projects:\n",
    "    truth_df[p] = pd.read_csv(p + \"_concat.csv\", index_col=None)\n",
    "    truth_df[p]['time'] = pd.to_datetime(truth_df[p]['time'])\n",
    "    print(truth_df[p].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pcmsolver', 'rmg-py', 'hoomd', 'amber', 'xenon', 'mdanalysis', 'libmesh', 'abinit', 'lammps'])\n"
     ]
    }
   ],
   "source": [
    "print(truth_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdanalysis 48 3344\n",
      "mdanalysis 48 3344 [352, 339, 392, 355, 426, 429, 403, 648]\n",
      "libmesh 59 8679\n",
      "libmesh 59 8679 [1125, 1326, 1339, 947, 1062, 983, 1897]\n",
      "lammps 93 7514\n",
      "lammps 93 7514 [807, 883, 914, 1180, 855, 754, 837, 1284]\n"
     ]
    }
   ],
   "source": [
    "from dateutil.parser import parse as dt_parse\n",
    "os.chdir(main_path)\n",
    "preprocessed_dfs = {}\n",
    "#temp_p_df = {}\n",
    "#for k, v in p_df.items():\n",
    "#    if k != \"pcmsolver\":\n",
    "#        temp_p_df[k] = v\n",
    "for p, releases in p_df.items():\n",
    "    preprocessed_dfs[p] = []\n",
    "    temp_df_total = truth_df[p]\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_releases = [[r[0], dt_parse(r[1]).replace(tzinfo=None)] for r in releases]\n",
    "    temp_releases = sorted(temp_releases, key=lambda r: r[1])\n",
    "    #print([r[1] for r in temp_releases])\n",
    "    #sorted_r = list(reversed(releases))\n",
    "    #print([r[1] for r in sorted_r])\n",
    "    print(p, len(temp_releases), temp_df_total.shape[0])\n",
    "    for i in range(len(temp_releases)):\n",
    "        r = temp_releases[i]\n",
    "        time_r = r[1]\n",
    "        #time_r = dt_parse(r[1]).replace(tzinfo=None)\n",
    "        #print(truth_df[p]['time'].iloc[0], \"hello\", time_r)\n",
    "        #print(time_r, temp_df_total['time'].iloc[0])\n",
    "        if temp_df_total['time'].iloc[0] > time_r:\n",
    "            continue\n",
    "        else:\n",
    "            if temp_df.shape[0] > 0:\n",
    "                temp_df = pd.concat([temp_df, temp_df_total[temp_df_total['time'] < time_r]])\n",
    "            else:\n",
    "                #print(temp_df_total['time'] <= time_r)\n",
    "                temp_df = temp_df_total[temp_df_total['time'] < time_r]\n",
    "            temp_df_total = temp_df_total[temp_df_total['time'] >= time_r]\n",
    "            if temp_df.shape[0] > (float(truth_df[p].shape[0]) * 0.1):\n",
    "                preprocessed_dfs[p].append(temp_df)\n",
    "                temp_df.to_csv(\"preprocessed_commits/%s/%s_%s.csv\" % (p, p, str(len(preprocessed_dfs[p]))), index=False)\n",
    "                temp_df = pd.DataFrame()\n",
    "            else:\n",
    "                if i == len(temp_releases) - 1:\n",
    "                    preprocessed_dfs[p][-1] = pd.concat([preprocessed_dfs[p][-1], temp_df])\n",
    "                    preprocessed_dfs[p][-1].to_csv(\"preprocessed_commits/%s/%s_%s.csv\" % (p, p, str(len(preprocessed_dfs[p]))), index=False)\n",
    "            #print(p, temp_df.shape, time_r, len(preprocessed_dfs[p]), preprocessed_dfs[p][-1].shape)\n",
    "                elif temp_df_total.shape[0] < 1:\n",
    "                    preprocessed_dfs[p][-1] = pd.concat([preprocessed_dfs[p][-1], temp_df])\n",
    "                    preprocessed_dfs[p][-1].to_csv(\"preprocessed_commits/%s/%s_%s.csv\" % (p, p, str(len(preprocessed_dfs[p]))), index=False)\n",
    "                    break\n",
    "    print(p, len(temp_releases), truth_df[p].shape[0], [x.shape[0] for x in preprocessed_dfs[p]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522 523 523 272\n",
      "1165 1165 1019 1538\n"
     ]
    }
   ],
   "source": [
    "for p in ['pcmsolver', 'amber']:\n",
    "    temp_dfs = []\n",
    "    if p == 'pcmsolver':\n",
    "        len_df = preprocessed_dfs[p][0].shape[0]\n",
    "        temp_dfs = [preprocessed_dfs[p][0][:int(len_df/3)], \n",
    "                    preprocessed_dfs[p][0][int(len_df/3):int(2*len_df/3)],\n",
    "                    preprocessed_dfs[p][0][int(2*len_df/3):],\n",
    "                    preprocessed_dfs[p][1]]\n",
    "        print(temp_dfs[0].shape[0],temp_dfs[1].shape[0], temp_dfs[2].shape[0], temp_dfs[3].shape[0])\n",
    "    else:\n",
    "        len_df = preprocessed_dfs[p][0].shape[0]\n",
    "        temp_dfs = [preprocessed_dfs[p][0][:int(len_df/2)], \n",
    "                    preprocessed_dfs[p][0][int(len_df/2):],\n",
    "                    preprocessed_dfs[p][1],preprocessed_dfs[p][2]]\n",
    "        print(temp_dfs[0].shape[0],temp_dfs[1].shape[0], temp_dfs[2].shape[0], temp_dfs[3].shape[0])\n",
    "    index = 1\n",
    "    for df in temp_dfs:\n",
    "        df.to_csv(\"preprocessed_commits/%s/%s_%s.csv\" % (p, p, index), index=False)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 10, 23, 13, 54, 16)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromtimestamp(int(float('1540317256.69321'))).replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 1844\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "2315 2315\n"
     ]
    }
   ],
   "source": [
    "five_projects = ['pcmsolver', 'rmg-py', 'hoomd', 'amber', 'xenon']\n",
    "os.chdir(main_path)\n",
    "for p in five_projects:\n",
    "    p_files = [filename for filename in os.listdir('raw') if filename.startswith(p)]\n",
    "    fr_files = [filename for filename in os.listdir('fastread') if filename.startswith(p)]\n",
    "    raw_df = pd.read_csv(\"auto/\" + p_files[0], index_col=None)\n",
    "    #raw_df['Year'] = pd.to_datetime(raw_df['Year'])\n",
    "    #raw_df = raw_df[-5000:]\n",
    "    #raw_df.to_csv(\"preprocessed/raw_\" + p + \".csv\", index=False)\n",
    "    fr_df = pd.read_csv(\"fastread/\" + fr_files[0], index_col=None)\n",
    "    print(raw_df.shape[0], fr_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcmsolver 1844\n",
      "                index  hash  time  message  keyword_buggy  human_buggy\n",
      "fastread_buggy                                                        \n",
      "0                1650  1650  1650     1650              0            0\n",
      "1                 194   194   194      194              0            0\n",
      "rmg-py 5000\n",
      "                index  hash  time  message  keyword_buggy  human_buggy\n",
      "fastread_buggy                                                        \n",
      "0                4347  4347  4347     4347              0            0\n",
      "1                 653   653   653      653              0            0\n",
      "hoomd 5000\n",
      "                index  hash  time  message  keyword_buggy  human_buggy\n",
      "fastread_buggy                                                        \n",
      "0                4236  4236  4236     4236              0            0\n",
      "1                 764   764   764      764              0            0\n",
      "amber 5000\n",
      "                index  hash  time  message  keyword_buggy  human_buggy\n",
      "fastread_buggy                                                        \n",
      "0                4191  4191  4191     4191              0            0\n",
      "1                 809   809   809      809              0            0\n",
      "xenon 2315\n",
      "                index  hash  time  message  keyword_buggy  human_buggy\n",
      "fastread_buggy                                                        \n",
      "0                1644  1644  1644     1644              0            0\n",
      "1                 671   671   671      671              0            0\n"
     ]
    }
   ],
   "source": [
    "five_pdf = {}\n",
    "projects = ['mdanalysis', 'libmesh', 'lammps']\n",
    "\n",
    "for p in projects:\n",
    "    five_pdf[p] = []\n",
    "    timestamps = []\n",
    "    hashvalues = []\n",
    "    col_names = ['hash', 'time', 'message', 'keyword_buggy', 'human_buggy', 'fastread_buggy']\n",
    "    p_df_final = pd.DataFrame(columns=col_names)\n",
    "    p_df_concat = pd.read_csv(\"fastread/\" + p + \"_fast_labeled.csv\")\n",
    "    p_df_concat['fr_labels'] = pd.Series(np.where(p_df_concat.code == 'yes', 1, 0), p_df_concat.index)\n",
    "    p_df_fast = pd.read_csv(\"raw/\" + p + \"_fast_labeled.csv\")\n",
    "    p_df_fast['Year'] = pd.to_datetime(p_df_fast['Year'])\n",
    "    for h in p_df_concat['Document Title'].values.tolist():\n",
    "        if len(h) > 7:\n",
    "            h = h[:7]\n",
    "        val = p_df_fast[p_df_fast['Document Title'].str.startswith(h)]\n",
    "        if not val.empty:\n",
    "            for i, row in val.iterrows():\n",
    "                if row['Document Title'][-2] != \".0\":            \n",
    "                    timestamps.append(val['Year'].values.tolist()[0])\n",
    "                    hashvalues.append(val['Document Title'].values.tolist()[0])\n",
    "        else: \n",
    "            print(h)\n",
    "    p_df_final['hash'] = hashvalues\n",
    "    p_df_final['time'] = timestamps\n",
    "    p_df_final['time'] = pd.to_datetime(p_df_final['time'])\n",
    "    p_df_final['message'] = p_df_concat['Abstract']\n",
    "    p_df_final['fastread_buggy'] = p_df_concat['fr_labels']\n",
    "    p_df_final = p_df_final.sort_values(by='time').reset_index()\n",
    "    p_df_final.to_csv(p + \"_concat.csv\", index=False)\n",
    "    print(p, p_df_final.shape[0])\n",
    "    print(p_df_final.groupby('fastread_buggy').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = ['pcmsolver', 'rmg-py', 'hoomd', 'amber', 'xenon', 'mdanalysis', 'libmesh', 'abinit', 'lammps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def keyword_buggy():\n",
    "    for p in projects:\n",
    "        index = 0\n",
    "        all_commits = pd.read_csv(p + \"_concat.csv\", index_col=None)\n",
    "        print(all_commits.shape)\n",
    "        commits_messages = all_commits['message'].values.tolist()\n",
    "        buggy = [0]*all_commits.shape[0]\n",
    "        for i in range(1, all_commits.shape[0]):\n",
    "            m = commits_messages[i]\n",
    "            try:\n",
    "                words = word_tokenize(m)\n",
    "                stemmed_w = [ps.stem(w) for w in words]\n",
    "                bug_key_words = set([\"bug\", \"fix\", \"wrong\", \"error\", \"fail\", \"problem\", \"patch\"])\n",
    "                check = set(stemmed_w).intersection(bug_key_words)\n",
    "                if len(check) > 0:\n",
    "                    buggy[i] = 1\n",
    "            except TypeError:\n",
    "                index += 1\n",
    "                #print(\"TypeError\", i, p, m)\n",
    "                continue\n",
    "            except UnicodeDecodeError:\n",
    "                index += 1\n",
    "                #print(\"UDError\", i, p, words)\n",
    "                continue\n",
    "        print(index)\n",
    "            #print(stemmed_w)\n",
    "            \n",
    "                    #     Buggy commit hash              Clean commit hash\n",
    "                    #(all_commits.iloc[i-1]['hash'], all_commits.iloc[i]['hash']))\n",
    "        all_commits['keyword_buggy'] = buggy\n",
    "        all_commits.to_csv(p + \"_concat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hash', 'time', 'message', 'keyword_buggy', 'human_buggy',\n",
      "       'fastread_buggy'],\n",
      "      dtype='object')\n",
      "Index(['hash', 'time', 'message', 'keyword_buggy', 'human_buggy',\n",
      "       'fastread_buggy'],\n",
      "      dtype='object')\n",
      "Index(['hash', 'time', 'message', 'keyword_buggy', 'human_buggy',\n",
      "       'fastread_buggy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for p in projects:\n",
    "    all_commits = pd.read_csv(p + \"_concat.csv\", index_col=None)\n",
    "    while(all_commits.shape[1] > 6):\n",
    "        all_commits = all_commits.drop(all_commits.columns[0], axis=1)\n",
    "    print(all_commits.columns)\n",
    "    all_commits.to_csv(p + \"_concat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4816"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([2166, 652, 568, 1430])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
