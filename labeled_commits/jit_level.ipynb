{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_LINKS = {\n",
    "    \"mdanalysis\": {\n",
    "        \"url\": \"https://github.com/MDAnalysis/mdanalysis\",\n",
    "        \"lang\": \"python\"\n",
    "    },\n",
    "    \"libmesh\": {\n",
    "        \"url\": \"https://github.com/libMesh/libmesh\",\n",
    "        \"lang\": \"C\"\n",
    "    },\n",
    "    \"lammps\": {\n",
    "        \"url\": \"https://github.com/lammps/lammps\",\n",
    "        \"lang\": \"C\"\n",
    "    }, \n",
    "    \"abinit\": {\n",
    "        \"url\": \"https://github.com/abinit/abinit\",\n",
    "        \"lang\": \"fortran\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shlex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdb import set_trace\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "from pathlib import Path\n",
    "import understand as und\n",
    "import subprocess as sp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "main_path = os.getcwd()\n",
    "print(main_path)\n",
    "p_df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _os_cmd(cmd, verbose=False):\n",
    "        \"\"\"\n",
    "        Run a command on the shell\n",
    "        Parameters\n",
    "        ----------\n",
    "        cmd: str\n",
    "            A command to run.\n",
    "        \"\"\"\n",
    "        cmd = shlex.split(cmd)\n",
    "        with sp.Popen(cmd, stdout=sp.PIPE, stderr=sp.DEVNULL) as p:\n",
    "            out, err = p.communicate()\n",
    "\n",
    "        if verbose:\n",
    "            print(out)\n",
    "            print(err)\n",
    "        return out, err\n",
    "\n",
    "def _files_changed_in_git_diff(hash_1, hash_2):\n",
    "        \"\"\"\n",
    "        Get a list of all the files changed between two hashes\n",
    "        Parameters\n",
    "        ----------\n",
    "        hash_1 : str\n",
    "            Commit hash 1.\n",
    "        hash_2 : bool\n",
    "            Commit hash 2.\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]:\n",
    "            A list of all files changed. For simplicity we only include *.py\n",
    "            *.F90, *.c, *.cpp, *.java.\n",
    "        \"\"\"\n",
    "        #project_path build_jit_datasets= \"/home/huyqt7/Projects/PhD/eager/data/projects/%s/\" % p\n",
    "        #os.chdir(project_path)\n",
    "        out, __ = _os_cmd(\"git diff {}..{} --name-only\".format(hash_1, hash_2))\n",
    "        files_changed = []\n",
    "        for f in out.splitlines():\n",
    "            if f and \"__init__.py\" not in str(f):\n",
    "                type_f = str(f)[2:-1].split(\".\")\n",
    "                #print(str(f), type_f)\n",
    "                if type_f:\n",
    "                    if type_f[-1] in set([\"c\", \"cc\", \"cxx\", \"C\", \"cpp\", \"CPP\", \"h\", \"H\", \n",
    "                                   \"f90\", \"f\", \"f77\", \"f03\", \"f95\", \"for\", \"ftn\",\n",
    "                                   \"F9build_jit_datasets0\", \"F\", \"F77\", \"F03\", \"F95\", \"For\", \"Ftn\", \n",
    "                                   \"py\", \"java\"]):\n",
    "                        file_name = f.decode(\"utf-8\") \n",
    "                        files_changed.append(file_name)\n",
    "                        #print(file_name)\n",
    "                        #if type_f[-1][0] == \"F\":\n",
    "                        #    file_name[-4] == \"f\"\n",
    "                        #    files_changed.append(file_name)'''\n",
    "        return files_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jit_metrics_building(p, commit_hash, delta_files, udb_path):\n",
    "    str_files = \" \"\n",
    "    for f in delta_files:\n",
    "        str_files += str(f) + \" \" \n",
    "    re_commits = []\n",
    "    commit_cmd = \"git reset --hard {}\".format(commit_hash)\n",
    "    sp.call(commit_cmd, shell=True)\n",
    "    path = udb_path\n",
    "    if REPO_LINKS[p][\"lang\"] == \"fortran\":\n",
    "        cmd = \"und create -db {} -languages Fortran python C++ add {}\".format(str(path), str_files)\n",
    "        sp.call(cmd, shell=True)\n",
    "        cmd = \"und -db {} settings -FileTypes .F90=Fortran .F77=Fortran .F03=Fortran .F95=Fortran .F=Fortran\".format(str(path))\n",
    "        sp.call(cmd, shell=True)\n",
    "        cmd = \"und -db {} analyze -all\".format(str(path))\n",
    "    elif REPO_LINKS[p][\"lang\"] == \"python\":\n",
    "        cmd = \"und create -db {} -languages python add {} analyze -all\".format(\n",
    "            str(path), str_files)\n",
    "    elif REPO_LINKS[p][\"lang\"] == \"C\":\n",
    "        cmd = \"und create -db {} -languages C++ python add {} analyze -all\".format(\n",
    "            str(path), str_files)\n",
    "    elif REPO_LINKS[p][\"lang\"] == \"Java\":\n",
    "        cmd = \"und create -db {} -languages Java add {} analyze -all\".format(\n",
    "            str(path), str_files)\n",
    "    sp.call(cmd, shell=True)\n",
    "        #print(commit_cmd, cmd, dfs[index], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_udb_temp_folders(p):\n",
    "    project_path = \"/home/huyqt7/Projects/PhD/eager/data/projects/%s/\" % p\n",
    "    os.chdir(project_path)\n",
    "    cwd = Path(os.getcwd())\n",
    "    udb_path = cwd.joinpath(\".temp\", \"udb\")\n",
    "    if not udb_path.is_dir():\n",
    "        os.makedirs(udb_path)\n",
    "    return project_path, udb_path\n",
    "    \n",
    "def get_und_type(udb_path, p, udb_type):\n",
    "    udb_file_path = udb_path.joinpath(\"{}_{}.udb\".format(p, udb_type))\n",
    "    return udb_file_path\n",
    "\n",
    "def get_buggy_clean_pairs(main_path, p, index, type_label):\n",
    "    type_label = type_label.split(\"_\")[0] + \"_buggy\"\n",
    "    c_fname = \"%s/preprocessed_commits/%s/%s_%s.csv\" % (main_path, p, p, index + 1)\n",
    "    df_commits = pd.read_csv(c_fname, index_col=None)\n",
    "    fixed_bugs_indices = df_commits[df_commits[type_label] == 1].index.values.tolist()\n",
    "    fixed_bugs_indices = [idx for idx in fixed_bugs_indices if idx > 1]\n",
    "    previous_bugs_indices = [idx-1 for idx in fixed_bugs_indices]\n",
    "    fixed_bugs = df_commits.loc[fixed_bugs_indices]['hash'].values.tolist()\n",
    "    previous_bugs = df_commits.loc[previous_bugs_indices]['hash'].values.tolist()\n",
    "    return zip(previous_bugs, fixed_bugs)\n",
    "\n",
    "def update_changed_df(udb_path, files_changed, type_label, f_status):\n",
    "    curr_udb = und.open(udb_path)\n",
    "    metrics_df = pd.DataFrame() \n",
    "    for file in curr_udb.ents(\"File\"):\n",
    "        # print directory name\n",
    "        for f_c in files_changed:\n",
    "            if str(file) in f_c:\n",
    "                metrics = file.metric(file.metrics())\n",
    "                metrics[\"File\"] = f_c\n",
    "                metrics[type_label] = f_status\n",
    "                metrics_df = metrics_df.append(pd.Series(metrics), \n",
    "                                               ignore_index=True)\n",
    "    # Purge und file\n",
    "    curr_udb.close()\n",
    "    _os_cmd(\"rm {}\".format(udb_path))\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_jit_datasets(type_label):\n",
    "    for p in REPO_LINKS.keys():\n",
    "        commits_dir = main_path + \"/preprocessed_commits/\" + p\n",
    "        files = [filename for filename in os.listdir(commits_dir) if filename.endswith(\".csv\")]\n",
    "        files = sorted(files)\n",
    "        no_versions = len(files)\n",
    "        p_path, udb_folder = create_udb_temp_folders(p)\n",
    "        print(os.getcwd())\n",
    "        print(p, end=\" \")\n",
    "        for index in range(no_versions):\n",
    "            print(index, end=\" \")\n",
    "            buggy_clean_pairs = get_buggy_clean_pairs(main_path, p, index, type_label)\n",
    "            metrics_dataframe = pd.DataFrame()\n",
    "            for buggy, cleanny in buggy_clean_pairs:\n",
    "                sp.call(\"git reset --hard master\", shell=True)\n",
    "                files_changed = _files_changed_in_git_diff(buggy, cleanny)\n",
    "                if files_changed:\n",
    "                    # ------------------------------------------------------------------\n",
    "                    # ---------------------- BUGGY FILES METRICS -----------------------\n",
    "                    # ------------------------------------------------------------------\n",
    "                    # Create a understand file for this hash\n",
    "                    udb_path_buggy = str(get_und_type(udb_folder, p, \"buggy\"))\n",
    "                    jit_metrics_building(p, buggy, files_changed, udb_path_buggy)\n",
    "                    temp_df_1 = update_changed_df(udb_path_buggy, files_changed, type_label, 1)\n",
    "\n",
    "\n",
    "                    sp.call(\"git reset --hard master\", shell=True)\n",
    "                    # ------------------------------------------------------------------\n",
    "                    # ---------------------- CLEAN FILES METRICS -----------------------\n",
    "                    # ------------------------------------------------------------------\n",
    "                    # Create a understand p, file for this hash\n",
    "                    udb_path_cleanny = str(get_und_type(udb_folder, p, \"cleanny\"))\n",
    "                    jit_metrics_building(p, cleanny, files_changed, udb_path_cleanny)\n",
    "                    temp_df_2 = update_changed_df(udb_path_cleanny, files_changed, type_label, 0)\n",
    "                    metrics_dataframe = pd.concat([metrics_dataframe, temp_df_1, temp_df_2], \n",
    "                                                  ignore_index=True)\n",
    "                    #print(files_changed, metrics_dataframe.shape, temp_df_1.shape[0], temp_df_2.shape[0])\n",
    "                \n",
    "            m_fname = \"%s/release_level/%s/%s_%s_jit_file_metrics.csv\" % (main_path, p, p, index)\n",
    "            columns_order = ['File'] + [a for a in metrics_dataframe.columns if a not in ['File', 'Bugs']] + ['Bugs']\n",
    "            metrics_dataframe = metrics_dataframe.reindex(columns=(columns_order))\n",
    "            #metrics_dataframe = metrics_dataframe.drop_duplicates(subset='File', keep=\"last\").reset_index()\n",
    "            metrics_dataframe = metrics_dataframe.dropna()\n",
    "            print(metrics_dataframe.shape, end=\", \")\n",
    "            metrics_dataframe.to_csv(m_fname)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(udb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_jit_datasets(\"keyword_buggy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 1, 2, 3, 4]\n",
    "b = [1, 3, 5, 7, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, shlex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdb import set_trace\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "from pathlib import Path\n",
    "import understand as und\n",
    "import subprocess as sp\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "main_path = os.getcwd()\n",
    "#print(main_path)\n",
    "p_df = {}\n",
    "\n",
    "REPO_LINKS = {\n",
    "    \"mdanalysis\": {\n",
    "        \"url\": \"https://github.com/MDAnalysis/mdanalysis\",\n",
    "        \"lang\": \"python\"\n",
    "    },\n",
    "    \"libmesh\": {\n",
    "        \"url\": \"https://github.com/libMesh/libmesh\",\n",
    "        \"lang\": \"C\"\n",
    "    },\n",
    "    \"lammps\": {\n",
    "        \"url\": \"https://github.com/lammps/lammps\",\n",
    "        \"lang\": \"C\"\n",
    "    }, \n",
    "    \"abinit\": {\n",
    "        \"url\": \"https://github.com/abinit/abinit\",\n",
    "        \"lang\": \"fortran\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _os_cmd(cmd, verbose=False):\n",
    "    \"\"\"\n",
    "    Run a command on the shell\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmd: str\n",
    "        A command to run.\n",
    "    \"\"\"\n",
    "    cmd = shlex.split(cmd)\n",
    "    with sp.Popen(cmd, stdout=sp.PIPE, stderr=sp.DEVNULL) as p:\n",
    "        out, err = p.communicate()\n",
    "\n",
    "    if verbose:\n",
    "        print(out)\n",
    "        print(err)\n",
    "    return out, err\n",
    "\n",
    "def _files_changed_in_git_diff(hash_1, hash_2):\n",
    "    \"\"\"\n",
    "    Get a list of all the files changed between two hashes\n",
    "    Parameters\n",
    "    ----------\n",
    "    hash_1 : str\n",
    "        Commit hash 1.\n",
    "    hash_2 : bool\n",
    "        Commit hash 2.\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]:\n",
    "        A list of all files changed. For simplicity we only include *.py\n",
    "        *.F90, *.c, *.cpp, *.java.\n",
    "    \"\"\"\n",
    "    #project_path build_jit_datasets= \"/home/huyqt7/Projects/PhD/eager/data/projects/%s/\" % p\n",
    "    #os.chdir(project_path)\n",
    "    out, __ = _os_cmd(\"git diff {}..{} --name-only\".format(hash_1, hash_2))\n",
    "    files_changed = []\n",
    "    for f in out.splitlines():\n",
    "        if f and \"__init__.py\" not in str(f):\n",
    "            type_f = f.decode(\"utf-8\").split(\".\")\n",
    "            if type_f:\n",
    "                if type_f[-1] in set([\"c\", \"cc\", \"C\", \"cpp\", \"CPP\", \"h\", \n",
    "                                \"f90\", \"f\", \"f77\", \"f03\", \"f95\", \"for\", \"ftn\",\n",
    "                                \"F90\", \"F\", \"F77\", \"F03\", \"F95\", \n",
    "                                \"py\", \"java\"]):\n",
    "                    file_name = f.decode(\"utf-8\") \n",
    "                    files_changed.append(file_name)\n",
    "                    #if type_f[-1][0] == \"F\":\n",
    "                    #    file_name[-4] == \"f\"\n",
    "                    #    files_changed.append(file_name)'''\n",
    "    return files_changed\n",
    "\n",
    "def jit_metrics_building(p, commit_hash, delta_files, udb_path):\n",
    "    str_files = \" \"\n",
    "    for f in delta_files:\n",
    "        str_files += str(f) + \" \"\n",
    "    re_commits = []\n",
    "    commit_cmd = \"git reset --hard {}\".format(commit_hash)\n",
    "    sp.call(commit_cmd, shell=True)\n",
    "    path = udb_path\n",
    "    if REPO_LINKS[p][\"lang\"] == \"fortran\":\n",
    "        cmd = \"und create -db {} -languages Fortran python \".format(str(path))\n",
    "        cmd += \"settings -FileTypes .F90=Fortran .F77=Fortran .F03=Fortran .F95=Fortran .F=Fortran \"\n",
    "        cmd += \"add {} analyze -all\".format(str_files)\n",
    "    elif REPO_LINKS[p][\"lang\"] == \"python\":\n",
    "        cmd = \"und create -db {} -languages python add {} analyze -all\".format(\n",
    "            str(path), str_files)\n",
    "    elif REPO_LINKS[p][\"lang\"] == \"C\":\n",
    "        cmd = \"und create -db {} -languages C++ python add {} analyze -all\".format(\n",
    "            str(path), str_files)\n",
    "    elif REPO_LINKS[p][\"lang\"] == \"Java\":\n",
    "        cmd = \"und create -db {} -languages Java add {} analyze -all\".format(\n",
    "            str(path), str_files)\n",
    "    sp.call(cmd, shell=True)\n",
    "        #print(commit_cmd, cmd, dfs[index], p)\n",
    "\n",
    "def create_udb_temp_folders(p):\n",
    "    project_path = \"/Users/huytu/Projects/eager/data/%s/\" % p\n",
    "    os.chdir(project_path)\n",
    "    cwd = Path(os.getcwd())\n",
    "    udb_path = cwd.joinpath(\".temp\", \"udb\")\n",
    "    if not udb_path.is_dir():\n",
    "        os.makedirs(udb_path)\n",
    "    return project_path, udb_path\n",
    "    \n",
    "def get_und_type(udb_path, p, type_label, udb_type):\n",
    "    udb_file_path = udb_path.joinpath(\"{}_{}_{}.udb\".format(p, type_label, udb_type))\n",
    "    return udb_file_path\n",
    "\n",
    "def get_buggy_clean_pairs(main_path, p, index, type_label):\n",
    "    type_label = type_label.split(\"_\")[0] + \"_buggy\"\n",
    "    c_fname = \"%s/preprocessed_commits/%s/%s_%s.csv\" % (main_path, p, p, index + 1)\n",
    "    df_commits = pd.read_csv(c_fname, index_col=None)\n",
    "    fixed_bugs_indices = df_commits[df_commits[type_label] == 1].index.values.tolist()\n",
    "    fixed_bugs_indices = [idx for idx in fixed_bugs_indices if idx > 1]\n",
    "    previous_bugs_indices = [idx-1 for idx in fixed_bugs_indices]\n",
    "    fixed_bugs = df_commits.loc[fixed_bugs_indices]['hash'].values.tolist()\n",
    "    previous_bugs = df_commits.loc[previous_bugs_indices]['hash'].values.tolist()\n",
    "    return zip(previous_bugs, fixed_bugs)\n",
    "\n",
    "def update_changed_df(udb_path, files_changed, type_label, commit_hash, f_status):\n",
    "    curr_udb = und.open(udb_path)\n",
    "    metrics_df = pd.DataFrame() \n",
    "    for file in curr_udb.ents(\"File\"):\n",
    "        # print directory name\n",
    "        for f_c in files_changed:\n",
    "            if str(file) in f_c:\n",
    "                metrics = file.metric(file.metrics())\n",
    "                metrics[\"File\"] = f_c\n",
    "                metrics[\"Hash\"] = commit_hash\n",
    "                metrics[type_label] = f_status\n",
    "                metrics_df = metrics_df.append(pd.Series(metrics), \n",
    "                                               ignore_index=True)\n",
    "    # Purge und file\n",
    "    curr_udb.close()\n",
    "    _os_cmd(\"rm {}\".format(udb_path))\n",
    "    return metrics_df\n",
    "\n",
    "def build_jit_datasets(type_label, p):\n",
    "    commits_dir = main_path + \"/preprocessed_commits/\" + p\n",
    "    files = [filename for filename in os.listdir(commits_dir) if filename.endswith(\".csv\")]\n",
    "    files = sorted(files)\n",
    "    no_versions = len(files)\n",
    "    p_path, udb_folder = create_udb_temp_folders(p)\n",
    "    print(os.getcwd())\n",
    "    for index in range(no_versions):\n",
    "        m_fname = \"%s/release_level/%s/%s_%s_%s_jit_file.csv\" % (main_path, p, p, index, type_label.split(\"_\")[0])\n",
    "        print(index, end=\" \")\n",
    "        buggy_clean_pairs = get_buggy_clean_pairs(main_path, p, index, type_label)\n",
    "        metrics_dataframe = pd.DataFrame()\n",
    "        for buggy, cleanny in buggy_clean_pairs:\n",
    "            sp.call(\"git reset --hard master\", shell=True)\n",
    "            files_changed = _files_changed_in_git_diff(buggy, cleanny)\n",
    "            if files_changed:\n",
    "                # ------------------------------------------------------------------\n",
    "                # ---------------------- BUGGY FILES METRICS -----------------------\n",
    "                # ------------------------------------------------------------------\n",
    "                # Create a understand file for this hash\n",
    "                udb_path_buggy = str(get_und_type(udb_folder, p, type_label, \"buggy\"))\n",
    "                jit_metrics_building(p, buggy, files_changed, udb_path_buggy)\n",
    "                temp_df_1 = update_changed_df(udb_path_buggy, files_changed, type_label, buggy, 1)\n",
    "\n",
    "\n",
    "                #sp.call(\"git reset --hard master\")\n",
    "                # ------------------------------------------------------------------\n",
    "                # ---------------------- CLEAN FILES METRICS -----------------------\n",
    "                # ------------------------------------------------------------------\n",
    "                # Create a understand p, file for this hash\n",
    "                udb_path_cleanny = str(get_und_type(udb_folder, p, type_label, \"cleanny\"))\n",
    "                jit_metrics_building(p, cleanny, files_changed, udb_path_cleanny)\n",
    "                temp_df_2 = update_changed_df(udb_path_cleanny, files_changed, type_label, cleanny, 0)\n",
    "                metrics_dataframe = pd.concat([metrics_dataframe, temp_df_1, temp_df_2], \n",
    "                                                ignore_index=True)\n",
    "                print(files_changed, metrics_dataframe.shape, temp_df_1.shape[0], temp_df_2.shape[0])\n",
    "\n",
    "        columns_order = ['File', 'Hash'] + [a for a in metrics_dataframe.columns if a not in ['File', 'Hash', type_label]] + [type_label]\n",
    "        metrics_dataframe = metrics_dataframe.reindex(columns=(columns_order))\n",
    "        #metrics_dataframe = metrics_dataframe.drop_duplicates(subset='File', keep=\"last\").reset_index()\n",
    "        #metrics_dataframe = metrics_dataframe.dropna()\n",
    "        print(metrics_dataframe.shape, end=\" , \")\n",
    "        metrics_dataframe.to_csv(m_fname)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type = \"keyword_buggy\"\n",
    "project = \"abinit\"\n",
    "build_jit_datasets(file_type, project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
