{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys  \n",
    "\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "sys.stdout = stdout\n",
    "projects = ['abinit', 'lammps', 'mdanalysis', 'libmesh']\n",
    "#projects = ['mdanalysis', 'rmg-py', 'hoomd', 'lammps', 'amber', 'xenon', 'abinit', 'libmesh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for p in projects:\n",
    "    directory = p + \"/\"\n",
    "    files = [filename for filename in os.listdir(p) if filename.endswith(\".csv\")]\n",
    "    col_names = [\"hash\", 'time', 'message', 'buggy']\n",
    "    p_df = pd.DataFrame(columns=col_names)\n",
    "    for f in files:\n",
    "        f_loc = directory + f\n",
    "        temp_df = pd.read_csv(f_loc, index_col=False) \n",
    "        if p_df.shape[0] != 0:\n",
    "            p_df = pd.concat([p_df, temp_df])\n",
    "        else:\n",
    "            p_df = temp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abinit', 11258, 0.5210516965713271, 0.05830207978179339)\n",
      "('abinit', (5392, 4))\n",
      "('lammps', 26616, 0.7248271716260896, 0.011196350818992328)\n",
      "('lammps', (7324, 4))\n",
      "('mdanalysis', 5517, 0.401305057096248, 0.1025293586269196)\n",
      "('mdanalysis', (3303, 4))\n",
      "('libmesh', 9339, 0.0706713780918728, 0.1)\n",
      "('libmesh', (8679, 4))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abinit', 'lammps', 'mdanalysis', 'libmesh']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {}\n",
    "unique_entries = {}\n",
    "for p in projects:\n",
    "    unique_entries[p] = {}\n",
    "    directory = p + \"/\"\n",
    "    files = [filename for filename in os.listdir(p) if filename.endswith(\".csv\")]\n",
    "    col_names = [\"hash\", 'time', 'message', 'buggy']\n",
    "    p_df = pd.DataFrame(columns=col_names)\n",
    "    for f in files:\n",
    "        f_loc = directory + f\n",
    "        temp_df = pd.read_csv(f_loc, index_col=False) \n",
    "        if p_df.shape[0] != 0:\n",
    "            p_df = pd.concat([p_df, temp_df])\n",
    "        else:\n",
    "            p_df = temp_df\n",
    "    count_1 = 0\n",
    "    for index, row in p_df.iterrows():\n",
    "        if row['hash'] not in unique_entries[p].keys():\n",
    "            unique_entries[p][row['hash']] = [row['buggy'], 0]\n",
    "        else:\n",
    "            count_1 += 1\n",
    "            #print(row['buggy'], unique_entries[row['hash']][0])\n",
    "            if row['buggy'] != unique_entries[p][row['hash']][0]:\n",
    "                unique_entries[p][row['hash']][1] = 1 \n",
    "    count = 0\n",
    "    for key, value in unique_entries[p].items():\n",
    "        #print(value)\n",
    "        if value[1] == 1:\n",
    "            count += 1\n",
    "    print(p, p_df.shape[0], float(count_1)/p_df.shape[0], float(count)/count_1)\n",
    "    p_df = p_df.drop_duplicates(subset=['hash'], keep=\"last\").reset_index(drop=True)\n",
    "    df[p] = p_df \n",
    "    print(p, p_df.shape)\n",
    "    #p_df.to_csv(p + \"_human.csv\", index=False)\n",
    "    #df[p] = p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5392, 4)\n",
      "(7324, 4)\n",
      "(3303, 4)\n",
      "(8679, 4)\n"
     ]
    }
   ],
   "source": [
    "for p in projects:\n",
    "    print(df[p].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_df = {}\n",
    "for p in projects:\n",
    "    col_names = [\"Document Title\", 'Abstract', 'Year', 'PDF Link', 'label']\n",
    "    p_df = pd.DataFrame(columns=col_names)\n",
    "    temp_df = df[p] \n",
    "    p_df['Document Title'] = temp_df['hash']\n",
    "    p_df['Abstract'] = temp_df['message']\n",
    "    p_df['Year'] = [2018]*temp_df.shape[0]\n",
    "    p_df.to_csv(p + \"_fast.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def most_common(L):\n",
    "  groups = itertools.groupby(sorted(L))\n",
    "  def _auxfun((item, iterable)):\n",
    "    return len(list(iterable)), -L.index(item)\n",
    "  return max(groups, key=_auxfun)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = {}\n",
    "unique_entries = {}\n",
    "for p in projects:\n",
    "    unique_entries[p] = {}\n",
    "    directory = p + \"/\"\n",
    "    files = [filename for filename in os.listdir(p) if filename.endswith(\".csv\")]\n",
    "    col_names = [\"hash\", 'time', 'message', 'buggy']\n",
    "    p_df = pd.DataFrame(columns=col_names)\n",
    "    for f in files:\n",
    "        f_loc = directory + f\n",
    "        temp_df = pd.read_csv(f_loc, index_col=False) \n",
    "        if p_df.shape[0] != 0:\n",
    "            p_df = pd.concat([p_df, temp_df]).reset_index(drop=True)\n",
    "        else:\n",
    "            p_df = temp_df\n",
    "    for index, row in p_df.iterrows():\n",
    "        if row['hash'] not in unique_entries[p].keys():\n",
    "            unique_entries[p][row['hash']] = [row['buggy']]\n",
    "        else:\n",
    "            #print(row['buggy'], unique_entries[row['hash']][0])\n",
    "            unique_entries[p][row['hash']].append(row['buggy'])\n",
    "    \n",
    "    #p_df = p_df.drop_duplicates(subset=['hash'], keep=\"last\")\n",
    "    #p_df.to_csv(p + \"_human.csv\", index=False)\n",
    "    #df[p] = p_df\n",
    "    #print(p, p_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(unique_entries['abinit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abinit', 388, 287, 57, 389)\n",
      "('lammps', 276, 476, 9, 97)\n",
      "('mdanalysis', 458, 355, 41, 420)\n",
      "('libmesh', 720, 756, 102, 643)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for p in projects:\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    col_names = [\"hash\", 'time', 'message', 'buggy']\n",
    "    p_df = pd.DataFrame(columns=col_names)\n",
    "    p_df_fast = pd.read_csv(\"/home/huyqt7/Projects/PhD/data-collection/labeled_commits/fastread/\" + p + \"_fast_labeled.csv\")\n",
    "    #print(p_df_fast.shape[0], df[p].shape[0])\n",
    "    #no_p_df = p_df_fast[p_df_fast.code == \"undetermined\"]\n",
    "    #no_labels = no_p_df.shape[0]*[0]\n",
    "    #random_1_undetermined = random.sample(range(0, no_p_df.shape[0]), int(0.08 * no_p_df.shape[0]) + 1)\n",
    "    #for i in random_1_undetermined:\n",
    "    #    no_labels[i] = 1\n",
    "    #no_p_df['label'] = no_labels\n",
    "    p_df_fast = p_df_fast[p_df_fast.code != \"undetermined\"]\n",
    "    #print(p_df_fast.shape[0])\n",
    "    p_df_fast['label'] = pd.Series(np.where(p_df_fast.code == 'yes', 1, 0),\n",
    "          p_df_fast.index)\n",
    "    count = 0\n",
    "    yes_und = 0\n",
    "    actual = []\n",
    "    #predicted = p_df_fast['label'].values.tolist()\n",
    "    for index, row in p_df_fast.iterrows():\n",
    "        predicted.append(row['label'])\n",
    "        comparable_idx = list(df[p][df[p]['hash'] == row['Document Title']].index)\n",
    "        if not actual:\n",
    "            actual = df[p].loc[comparable_idx]['buggy'].values.tolist()\n",
    "        else:\n",
    "            actual.extend(df[p].loc[comparable_idx]['buggy'].values.tolist())\n",
    "    tn, fp, fn, tp = confusion_matrix(actual, predicted).ravel()\n",
    "    print(p, tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abinit', 395, 280, 44, 402)\n",
      "('libmesh', 584, 892, 58, 687)\n",
      "('lammps', 137, 615, 11, 95)\n",
      "('mdanalysis', 543, 270, 31, 430)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for p in df.keys():\n",
    "    p_df_fast = pd.read_csv(\"/home/huyqt7/Projects/PhD/data-collection/labeled_commits/fastread/\" + p + \"_fast_labeled.csv\")\n",
    "    p_df_fast = p_df_fast[p_df_fast.code != \"undetermined\"]\n",
    "    p_df_fast['label'] = pd.Series(np.where(p_df_fast.code == 'yes', 1, 0), p_df_fast.index)\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for index, row in p_df_fast.iterrows():\n",
    "        comparable_idx = list(df[p][df[p]['hash'] == row['Document Title']].index)\n",
    "        if not actual:\n",
    "            actual = df[p].loc[comparable_idx]['buggy'].values.tolist()\n",
    "        else:\n",
    "            actual.extend(df[p].loc[comparable_idx]['buggy'].values.tolist())\n",
    "        m = df[p].loc[comparable_idx]['message'].values.tolist()[0]\n",
    "        try:\n",
    "            words = word_tokenize(m)\n",
    "            stemmed_w = [ps.stem(w) for w in words]\n",
    "        except TypeError:\n",
    "            print(\"TypeError\", m)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"UDError\", words)\n",
    "            break\n",
    "        #print(stemmed_w)\n",
    "        bug_key_words = set([\"bug\", \"fix\", \"wrong\", \"error\", \"fail\", \"problem\", \"patch\"])\n",
    "        check = set(stemmed_w).intersection(bug_key_words)\n",
    "        if len(check) > 0:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "    tn, fp, fn, tp = confusion_matrix(actual, predicted).ravel()\n",
    "    print(p, tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0778bc0c3fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not list"
     ]
    }
   ],
   "source": [
    "\"('abinit', 3821, 586, 111, 394, 4912, 3791)\n",
    "('lammps', 6222, 994, 11, 97, 7324, 6466)\n",
    "('mdanalysis', 2308, 515, 57, 423, 3303, 2029)\n",
    "('libmesh', 6637, 1270, 126, 646, 8679, 6458)\"\n",
    "\n",
    "\"('abinit', 79, 367, 185, 78, 491, 1121, 3791)\n",
    "('lammps', 9, 280, 494, 5, 79, 858, 6466)\n",
    "('mdanalysis', 16, 481, 524, 18, 251, 1274, 2029)\n",
    "('libmesh', 27, 715, 701, 107, 698, 2221, 6458)\n",
    "\"\n",
    "a = [2, 3, 4, 5, 6]\n",
    "a[[1, 2]] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_unique_entries = {}\n",
    "for p in projects:\n",
    "    temp_unique_entries[p] = unique_entries[p]\n",
    "    for k in temp_unique_entries[p].keys():\n",
    "        temp_unique_entries[p][k] = most_common(unique_entries[p][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abinit', 4912)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0      4174     4174  4174\n",
      "1       738      738   738\n",
      "('lammps', 7324)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0      6750     6750  6750\n",
      "1       574      574   574\n",
      "('mdanalysis', 3303)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0      2513     2513  2513\n",
      "1       790      790   790\n",
      "('libmesh', 8679)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0      7253     7250  7253\n",
      "1      1426     1426  1426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for p in projects:\n",
    "    predicted = []\n",
    "    actual = []\n",
    "        \n",
    "    col_names = [\"hash\", 'message', 'time', 'buggy']\n",
    "    p_df = pd.DataFrame(columns=col_names)\n",
    "    p_df_fast = pd.read_csv(\"fastread/\" + p + \"_fast_labeled.csv\")\n",
    "    p_df_fast_labels = p_df_fast['label'].values\n",
    "    p_df_fast = p_df_fast.drop(['label', 'Year', 'PDF Link'], axis=1)\n",
    "    p_df_fast['label'] = p_df_fast_labels\n",
    "    #print(p_df_fast.shape[0])\n",
    "    no_p_df = p_df_fast[p_df_fast.code == \"undetermined\"]\n",
    "    \n",
    "    #no_p_df['label'] = no_p_df.shape[0]*[0]\n",
    "    p_df_fast = p_df_fast[p_df_fast.code != \"undetermined\"]\n",
    "    #print(p_df_fast.shape[0])\n",
    "    p_df_fast['label'] = pd.Series(np.where(p_df_fast.code == 'yes', 1, 0),\n",
    "          p_df_fast.index)\n",
    "    p_df_fast = p_df_fast.drop(['code'], axis=1)\n",
    "    no_p_df = no_p_df.drop(['code'], axis=1)\n",
    "    #p_df_fast = pd.concat([p_df_fast, no_p_df])\n",
    "    count = 0\n",
    "    yes_und = 0\n",
    "    p_df_index = 0\n",
    "    for index, row in p_df_fast.iterrows():\n",
    "        if temp_unique_entries[p][row['Document Title']] == 1 or row['label'] == 1:\n",
    "            row['label'] == 1\n",
    "        p_df.loc[p_df_index] = row.tolist()\n",
    "        p_df_index += 1\n",
    "            \n",
    "    for index, row in no_p_df.iterrows():\n",
    "        if temp_unique_entries[p][row['Document Title']] == 1:\n",
    "            row['label'] = 1\n",
    "        else:\n",
    "            row['label'] = 0\n",
    "        #print(row.tolist())\n",
    "        p_df.loc[p_df_index] = row.tolist()\n",
    "        p_df_index += 1\n",
    "    p_df.to_csv(p + \"_concat.csv\")\n",
    "    print(p, p_df.shape[0])\n",
    "    print(p_df.groupby('buggy').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "('abinit', 258, 1121, 0.23015165031222123)\n",
    "('lammps', 501, 858, 0.583916083916084)\n",
    "('mdanalysis', 472, 1274, 0.3704866562009419)\n",
    "('libmesh', 395, 1001, 0.3946053946053946)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amber', 967)\n",
      "       Document Title  Abstract  time\n",
      "label                                \n",
      "0                 158       158   158\n",
      "1                 809       809   809\n",
      "('hoomd', 881)\n",
      "       Document Title  Abstract  time\n",
      "label                                \n",
      "0                 117       117   117\n",
      "1                 764       764   764\n",
      "('pcmsolver', 328)\n",
      "       Document Title  Abstract  time\n",
      "label                                \n",
      "0                 134       134   134\n",
      "1                 194       194   194\n",
      "('rmg-py', 706)\n",
      "       Document Title  Abstract  time\n",
      "label                                \n",
      "0                  53        53    53\n",
      "1                 653       653   653\n",
      "('xenon', 921)\n",
      "       Document Title  Abstract  time\n",
      "label                                \n",
      "0                 250       250   250\n",
      "1                 671       671   671\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for p in projects:\n",
    "    predicted = []\n",
    "    actual = []\n",
    "        \n",
    "    col_names = [\"hash\", 'message', 'time', 'buggy']\n",
    "    p_df_fast = pd.DataFrame(columns=col_names)\n",
    "    p_df_fast = pd.read_csv(\"fastread/\" + p + \"_fast_labeled.csv\")\n",
    "    p_df_fast_labels = p_df_fast['label'].values\n",
    "    p_df_fast = p_df_fast[p_df_fast.code != \"undetermined\"]\n",
    "    p_df_fast = p_df_fast.drop(['label', 'Year', 'PDF Link'], axis=1)\n",
    "    p_df_fast['label'] = pd.Series(np.where(p_df_fast.code == 'yes', 1, 0),\n",
    "          p_df_fast.index)\n",
    "    p_df_fast = p_df_fast.drop(['code'], axis=1)\n",
    "    \n",
    "    p_df_fast.to_csv(p + \"_concat.csv\")\n",
    "    print(p, p_df_fast.shape[0])\n",
    "    print(p_df_fast.groupby('label').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for p in projects:\n",
    "    p_df_fast = pd.read_csv(p + \"_concat.csv\")\n",
    "    random_indexes = random.sample(range(1, p_df_fast.shape[0]), 100)\n",
    "    p_sample = p_df_fast.ix[random_indexes]\n",
    "    p_sample.to_csv(\"sanity_checks/\" + p +  \"_sample.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mdanalysis', 3344)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0.0    2513     2513  2513\n",
      "1.0     790      790   790\n",
      "('libmesh', 8679)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0      7253     7250  7253\n",
      "1      1426     1426  1426\n",
      "('abinit', 4912)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0      4174     4174  4174\n",
      "1       738      738   738\n",
      "('lammps', 7514)\n",
      "       hash  message  time\n",
      "buggy                     \n",
      "0.0    6750     6750  6750\n",
      "1.0     574      574   574\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "projects = ['mdanalysis', 'libmesh', 'abinit', 'lammps']\n",
    "#projects = ['mdanalysis']\n",
    "for p in projects:\n",
    "    timestamps = []\n",
    "    hashvalues = []\n",
    "    col_names = [\"hash\", 'message', 'time', 'buggy']\n",
    "    p_df_final = pd.DataFrame(columns=col_names)\n",
    "    p_df_concat = pd.read_csv(\"auto/\" + p + \"_concat.csv\")\n",
    "    p_df_fast = pd.read_csv(p + \"_human.csv\")\n",
    "    for h in p_df_concat['hash'].values.tolist():\n",
    "        if len(h) > 7:\n",
    "            h = h[:7]\n",
    "        val = p_df_fast[p_df_fast['hash'].str.startswith(h)]\n",
    "        if not val.empty:\n",
    "            for i, row in val.iterrows():\n",
    "                if row['hash'][-2] != \".0\":            \n",
    "                    timestamps.append(val['time'].values.tolist()[0])\n",
    "                    hashvalues.append(val['hash'].values.tolist()[0])\n",
    "        else: \n",
    "            print(h)\n",
    "    p_df_final['hash'] = hashvalues\n",
    "    p_df_final['time'] = timestamps\n",
    "    p_df_final['message'] = p_df_concat['message']\n",
    "    p_df_final['buggy'] = p_df_concat['buggy']\n",
    "    p_df_final.to_csv(p + \"_concat.csv\")\n",
    "    print(p, p_df_final.shape[0])\n",
    "    print(p_df_final.groupby('buggy').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
